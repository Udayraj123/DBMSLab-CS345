{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "runflag=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"workshop_dataset/workshop_dataset1/\";\n",
    "all_files = glob.glob(data_dir+\"*.json\")\n",
    "total_files = len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_json(all_files[0],orient='index',\n",
    "                convert_dates=False, #dont convert columns to dates \n",
    "                convert_axes=False, #dont convert index to dates\n",
    "        )\n",
    "# print(df.info())\n",
    "string = pd.io.sql.get_schema(df, 'twitter_table')\n",
    "# print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Something's wrong with dataset, total counts not matching! 111543 5\n",
      "Done scanning 113 files with total of 111543 records(987.11 records/file), in 35 seconds(3.20 files/sec)\n"
     ]
    }
   ],
   "source": [
    "# Test the data for consistencies. \n",
    "\n",
    "# Check Identical columns in all files and\n",
    "# Note Record Counts to tally with cassandra counts later\n",
    "if(runflag == 1):\n",
    "    i=0\n",
    "    df_shapes=[]\n",
    "    all_records = pd.DataFrame()\n",
    "    time_taken = 0\n",
    "    prev_time = time.time()\n",
    "    for filename in all_files:\n",
    "        df=pd.read_json(filename,orient='index',convert_dates=False,convert_axes=False)    \n",
    "        df_shapes.append(df.shape)\n",
    "        curr_set = set(df)\n",
    "        if(i>0):\n",
    "            all_records = pd.concat([all_records,df])\n",
    "            if(prev_set != curr_set):\n",
    "                runflag=1\n",
    "                print(filename, \"New columns found!\", curr_set - prev_set)\n",
    "                break\n",
    "        else:\n",
    "            all_records=df\n",
    "        prev_set = curr_set\n",
    "        i+=1\n",
    "        runflag=0\n",
    "    time_taken= time.time()-prev_time\n",
    "\n",
    "# Rename columns that are keywords\n",
    "all_records = all_records.rename(columns={'date':'tweet_date','datetime':'tweet_datetime'})\n",
    "# convert dates to datetime objects\n",
    "all_records['tweet_date'] = pd.to_datetime(all_records['tweet_date'])\n",
    "all_records['tweet_datetime'] = pd.to_datetime(all_records['tweet_datetime'])\n",
    "# convert lists to str as the python driver doesn't do it before utf-encode \n",
    "list_columns=['hashtags', 'keywords_processed_list', 'mentions', 'url_list']\n",
    "for c in list_columns:\n",
    "    all_records[c]=all_records[c].astype(str)\n",
    "# floats to ints coz pandas uses float which supports NoneTypes too, ints can't support this in pandas!\n",
    "# float_columns=['quoted_source_id', 'replyto_source_id', 'retweet_source_id']\n",
    "# for c in list_columns:\n",
    "#     all_records[c]=all_records[c].astype(int)\n",
    "# Changing Cassandra columns to float as NULL support required.\n",
    "total_records= sum([ x for x,y in df_shapes])\n",
    "total_records2= all_records.shape[0]\n",
    "if(total_records!=total_records2):\n",
    "    print(\"Error: Something's wrong with dataset, total counts not matching!\",total_records,total_records2)\n",
    "if(runflag==0):\n",
    "    print(\"Done scanning %d files with total of %d records(%.2f records/file), in %d seconds(%.2f files/sec)\" %\n",
    "          (total_files, total_records, float(total_records)/total_files,time_taken, float(total_files)/time_taken))\n",
    "\n",
    "# Ensure data types are correct in the dataframe-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_records = all_records.tail(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Set for index 14 nan\n",
      "None Set for index 16 nan\n",
      "\n",
      "None Set for index 16 nan\n",
      "None Set for index 18 nan\n",
      "\n",
      "None Set for index 14 nan\n",
      "None Set for index 16 nan\n",
      "\n",
      "None Set for index 14 nan\n",
      "None Set for index 16 nan\n",
      "None Set for index 18 nan\n",
      "\n",
      "None Set for index 14 nan\n",
      "None Set for index 16 nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cassandra.query import UNSET_VALUE\n",
    "\n",
    "#debuggin\n",
    "all_records = all_records.tail(1000)\n",
    "columns = all_records.columns.values # or list(df)\n",
    "insertrecords = all_records.as_matrix()\n",
    "# Handling NULLs -\n",
    "# https://www.datastax.com/dev/blog/python-driver-2-6-0-rc1-with-cassandra-2-2-features\n",
    "def handleNullInts(row):\n",
    "    for i,val in enumerate(row):\n",
    "        if(type(val)==float):\n",
    "            if np.isnan(val):\n",
    "                print(\"None Set for index\",i,val)\n",
    "                row[i]= UNSET_VALUE \n",
    "            else:\n",
    "                row[i]= int(row[i])\n",
    "    print('')\n",
    "\n",
    "for record in insertrecords:\n",
    "    handleNullInts(record)\n",
    "\n",
    "#     col=columns[i]\n",
    "#     print(col,\"\\t\", type(r), \"\\t\", r)\n",
    "# print(insertrecords[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent, execute_concurrent_with_args\n",
    "\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "session = cluster.connect('twitterdb') # keyspace name should be lowercase\n",
    "statement = session.prepare(\"insert into twitter_table (author, author_id, author_profile_image, author_screen_name, tweet_date, tweet_datetime, hashtags, keywords_processed_list, lang, like_count, location, media_list, mentions, quote_count, quoted_source_id, reply_count, replyto_source_id, retweet_count, retweet_source_id, sentiment, tid, tweet_text, type, url_list, verified) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n",
    "# Note that it will overwrite data without error (by design)\n",
    "# session.execute(statement,insertrecords[0])\n",
    "execute_concurrent_with_args(session, statement, insertrecords, concurrency=50)\n",
    "\n",
    "session.shutdown(); # for use by cqlsh\n",
    "cluster.shutdown();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
