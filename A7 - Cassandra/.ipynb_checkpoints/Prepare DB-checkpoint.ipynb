{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "runflag=1\n",
    "insertflag=1\n",
    "single_record=-10 # index of record\n",
    "max_num_records=12000#000\n",
    "not_applieds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"workshop_dataset/workshop_dataset1/\";\n",
    "all_files = glob.glob(data_dir+\"*.json\")\n",
    "total_files = len(all_files)\n",
    "# file=all_files[0]\n",
    "# df=pd.read_json(file,orient='index',\n",
    "#                 convert_dates=False, #dont convert columns to dates \n",
    "#                 convert_axes=False, #dont convert index to dates\n",
    "#         )\n",
    "# print(df.info())\n",
    "\n",
    "# print(df)\n",
    "# createTableString = pd.io.sql.get_schema(df, 'twitter_table')\n",
    "# print(createTableString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scanning 113 files with total of 12616 records(111.65 records/file), in 1.55 seconds(73.10 files/sec)\n"
     ]
    }
   ],
   "source": [
    "# Test the data for consistencies. \n",
    "\n",
    "# Check Identical columns in all files and\n",
    "# Note Record Counts to tally with cassandra counts later\n",
    "if(runflag == 1):\n",
    "    i=0\n",
    "    df_shapes=[]\n",
    "    all_records = pd.DataFrame()\n",
    "    time_taken = 0\n",
    "    prev_time = time.time()\n",
    "    for filename in all_files:\n",
    "        #the dtype is important for correctness. https://stackoverflow.com/questions/49627084/pandas-read-json-reads-large-integers-as-strings-incorrectly\n",
    "        df=pd.read_json(filename,orient='index',convert_dates=False,convert_axes=False,dtype={\"tid\": int})            \n",
    "        df_shapes.append(df.shape)\n",
    "        curr_set = set(df)\n",
    "        if(i>0):\n",
    "            all_records = pd.concat([all_records,df])\n",
    "            if(prev_set != curr_set):\n",
    "                runflag=1\n",
    "                print(filename, \"New columns found!\", curr_set - prev_set)\n",
    "                break\n",
    "        else:\n",
    "            all_records=df\n",
    "        runflag=0\n",
    "        if(all_records.shape[0] > max_num_records):\n",
    "            break\n",
    "        prev_set = curr_set\n",
    "        i+=1\n",
    "    time_taken= time.time()-prev_time\n",
    "\n",
    "    # Rename columns that are keywords\n",
    "    all_records = all_records.rename(columns={'date':'tweet_date','datetime':'tweet_datetime'})\n",
    "    # convert dates to datetime objects\n",
    "    all_records['tweet_date'] = pd.to_datetime(all_records['tweet_date'])\n",
    "    all_records['tweet_datetime'] = pd.to_datetime(all_records['tweet_datetime'])\n",
    "    all_records['verified']=all_records['verified'].astype(bool)\n",
    "    # convert lists to str as the python driver doesn't do it before utf-encode \n",
    "    # list_columns=['hashtags', 'keywords_processed_list', 'mentions', 'url_list']\n",
    "    # for c in list_columns:\n",
    "    #     all_records[c]=all_records[c].astype(str)\n",
    "\n",
    "total_records= sum([ x for x,y in df_shapes])\n",
    "total_records2= all_records.shape[0]\n",
    "if(total_records!=total_records2):\n",
    "    print(\"Error: Something's wrong with dataset, total counts not matching!\",total_records,total_records2)\n",
    "if(runflag==0):\n",
    "    print(\"Done scanning %d files with total of %d records(%.2f records/file), in %.2f seconds(%.2f files/sec)\" %\n",
    "          (total_files, total_records, float(total_records)/total_files,time_taken, float(total_files)/time_taken))\n",
    "\n",
    "# Ensure data types are correct in the dataframe-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.query import UNSET_VALUE\n",
    "# from cassandra.util import OrderedMap\n",
    "\n",
    "#debuggin\n",
    "all_records_tail = all_records.tail(max_num_records)\n",
    "all_columns = all_records_tail.columns.values # or list(df)\n",
    "insert_records = all_records_tail.as_matrix()\n",
    "# Handling NULLs -\n",
    "# https://www.datastax.com/dev/blog/python-driver-2-6-0-rc1-with-cassandra-2-2-features\n",
    "def handleNullInts(row):\n",
    "    for i,val in enumerate(row):\n",
    "#         if(type(val)==dict):\n",
    "#             Convert keys to integers\n",
    "#             row[i] = OrderedMap(row[i])\n",
    "        # floats to ints coz pandas uses float which supports NoneTypes too, ints can't support this in pandas!\n",
    "        # float_columns=['quoted_source_id', 'replyto_source_id', 'retweet_source_id']\n",
    "        # Changing Cassandra columns to float as NULL support required.\n",
    "        if(type(val)==float):\n",
    "            if np.isnan(val):\n",
    "#                 print(\"None Set for index\",i,val)\n",
    "                row[i]= UNSET_VALUE \n",
    "            else:                \n",
    "                row[i]= int(row[i])\n",
    "\n",
    "for record in insert_records:\n",
    "    handleNullInts(record)\n",
    "\n",
    "# print(insert_records[0])\n",
    "\n",
    "# ^^ Can use this migration method as well: https://datastax.github.io/python-driver/object_mapper.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(iquery) # Reloads modules in interactive shell\n",
    "# https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('tid', 'author', 'author_id', 'author_profile_image', 'author_screen_name', 'hashtags', 'keywords_processed_list', 'lang', 'like_count', 'location', 'media_list', 'mentions', 'quote_count', 'quoted_source_id', 'reply_count', 'replyto_source_id', 'retweet_count', 'retweet_source_id', 'sentiment', 'tweet_text', 'tweet_date', 'tweet_datetime', 'type', 'url_list', 'verified')\n",
      "tweets_by_tid 956677746704437248\n",
      "12000 --> 12000\n",
      "\n",
      "('author_screen_name', 'author', 'author_id', 'lang', 'location', 'tid', 'tweet_text', 'tweet_date', 'tweet_datetime', 'like_count', 'author_profile_image', 'hashtags', 'keywords_processed_list', 'media_list', 'mentions', 'quote_count', 'quoted_source_id', 'reply_count', 'replyto_source_id', 'retweet_count', 'retweet_source_id', 'sentiment', 'type', 'url_list', 'verified')\n",
      "tweets_by_author leopoldojr42\n",
      "12000 --> 12000\n",
      "\n",
      "('tweet_date', 'like_count', 'tweet_datetime', 'tid')\n",
      "tids_by_date 2018-01-25 00:00:00\n",
      "12000 --> 12000\n",
      "\n",
      "('location', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_location Malden, MA\n",
      "12000 --> 8271\n",
      "\n",
      "('hashtag', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_hashtag ThursdayThoughts\n",
      "12000 --> 14125\n",
      "\n",
      "('mention', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_mention Hoosiers1986\n",
      "12000 --> 14556\n",
      "\n",
      "('keyword', 'tid', 'like_count')\n",
      "tids_by_keyword corruption\n",
      "12000 --> 25346\n"
     ]
    }
   ],
   "source": [
    "# Create individual record arrays here\n",
    "import insert_queries as iquery # local file having insert statements\n",
    "insert_tables = iquery.columns.keys()\n",
    "table_records={}\n",
    "tally_counts={}\n",
    "custom_col_tables=['tids_by_keyword','tids_by_hashtag', 'tids_by_mention'];\n",
    "custom_col_listnames={\n",
    "    'keyword':'keywords_processed_list',\n",
    "    'hashtag':'hashtags',\n",
    "    'mention':'mentions'\n",
    "}\n",
    "\n",
    "def getindex(all_columns,c):\n",
    "    return np.where(all_columns == c)[0][0]\n",
    "\n",
    "def getcolname(all_columns,c):\n",
    "    if(c in custom_col_listnames.keys()):\n",
    "        return custom_col_listnames[c]\n",
    "    else:\n",
    "        return c\n",
    "special_table='popular_hashtags'\n",
    "for t in insert_tables:\n",
    "    if(t==special_table):\n",
    "        continue\n",
    "    else:\n",
    "        columns=iquery.columns[t]\n",
    "        print('')\n",
    "        print(columns)\n",
    "        partition_key = getindex(all_columns,getcolname(all_columns,columns[0]))    \n",
    "        filtered_records = insert_records[insert_records[:,partition_key] != None] #No floats used, so no need to check UNSET_VALUE\n",
    "        rem_indices = [ getindex(all_columns,c) for c in columns[1:] if c in all_columns]\n",
    "        if(t in custom_col_tables):    \n",
    "            list_records = filtered_records[:,partition_key]\n",
    "            expanded_records=[]\n",
    "            for row_no,row in enumerate(list_records):\n",
    "                for list_item in row:\n",
    "                    #Works only on first col\n",
    "                    expanded_records.append(np.concatenate([[list_item],filtered_records[row_no][rem_indices]]))\n",
    "            table_records[t]=expanded_records\n",
    "        else:\n",
    "            table_records[t]= filtered_records[:,[i for i in [partition_key]+rem_indices]]\n",
    "    print(t, table_records[t][0][0])\n",
    "    print(len(insert_records),\"-->\",len(table_records[t]))\n",
    "    tally_counts[t]=len(table_records[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Nope- df.sort_values(by=['tweet_datetime','hashtag'],ascending=[False,True])\n",
    "# df = pd.DataFrame(table_records['tids_by_hashtag'],columns=['hashtag','tid','tweet_date','tweet_datetime'])\n",
    "# date_col='tweet_date'\n",
    "# df = df[['hashtag',date_col]]\n",
    "# hashtag_counts={}\n",
    "# for day,rows in df.groupby(date_col,sort=True): #keys are sorted\n",
    "#     hashtag_counts[day] = rows['hashtag'].value_counts()\n",
    "# #     print(hashtag_counts[day])\n",
    "# #     print(rows.groupby('hashtag').count())\n",
    "\n",
    "# accum_sum = pd.DataFrame(hashtag_counts).fillna(0)\n",
    "# # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html\n",
    "# accum_sum = accum_sum.rolling(7,axis=1,min_periods=1).sum()\n",
    "# accum_sum.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# t = special_table\n",
    "# table_records[t]=gen_hashtag_stats(table_records['tids_by_hashtag'])\n",
    "# print(t, table_records[t][0][0])\n",
    "# print(len(insert_records),\"-->\",len(table_records[t]))\n",
    "# tally_counts[t]=len(table_records[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insertflag=1\n",
    "# single_record=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_profile_image</th>\n",
       "      <th>author_screen_name</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>keywords_processed_list</th>\n",
       "      <th>lang</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>replyto_source_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweet_source_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tid</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>type</th>\n",
       "      <th>url_list</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, author_id, author_profile_image, author_screen_name, tweet_date, tweet_datetime, hashtags, keywords_processed_list, lang, like_count, location, media_list, mentions, quote_count, quoted_source_id, reply_count, replyto_source_id, retweet_count, retweet_source_id, sentiment, tid, tweet_text, type, url_list, verified]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Does retweet_count == 0 mean mentions are null ? \n",
    "# temp_all_records = all_records[all_records['retweet_count']==0]\n",
    "# temp_all_records[temp_all_records['mentions']==None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting Records..\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent, execute_concurrent_with_args\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "session = cluster.connect('twitterdb') # keyspace name should be lowercase\n",
    "# added 'if not exists' at end\n",
    "\n",
    "# tweets_by_tid_stmt = session.prepare(iquery.statementstweets_by_tid)\n",
    "prepared_statements={}\n",
    "for t in insert_tables:\n",
    "    prepared_statements[t]=session.prepare(iquery.statements[t])\n",
    "    \n",
    "# Note that it will overwrite data without error (use 'IF NOT EXISTS' and check applied == False)\n",
    "# session.execute(statement,insert_records[0])\n",
    "\n",
    "if(insertflag==1):\n",
    "    print(\"Inserting Records..\")\n",
    "    prev_time = time.time()\n",
    "    if single_record > -1:\n",
    "#         print(\"media_list\", insert_records[single_record][11])\n",
    "        for t in insert_tables:\n",
    "            print(t)\n",
    "            results = session.execute(prepared_statements[t],table_records[t][single_record])\n",
    "    \n",
    "        result_set=[dotdict({'success':True,'result_or_exc' : results})]\n",
    "        total_inserted=1\n",
    "    else:\n",
    "        for t in insert_tables:\n",
    "            print(t)\n",
    "            result_set = list(execute_concurrent_with_args(session, prepared_statements[t], table_records[t], concurrency=50))\n",
    "        # One can iterate only once on this result_set!\n",
    "        # That is useful when you do pagination: https://datastax.github.io/php-driver/api/Cassandra/class.Rows/\n",
    "#         result_set = list(execute_concurrent_with_args(session, statement, insert_records, concurrency=50))\n",
    "        total_inserted = total_records\n",
    "    insertflag=0;\n",
    "    time_taken=time.time()-prev_time    \n",
    "            \n",
    "print(\"Total Time taken for Insertion: %.2f seconds\"% (time_taken) )\n",
    "# print(\"Time taken for Insertion: %.2f seconds, %.2f records/second \"% (time_taken, total_inserted/time_taken) )\n",
    "\n",
    "# print(\"Reviewing inserts..\")\n",
    "# for row_no,r in enumerate(result_set):\n",
    "#     if(r.success == True):\n",
    "#         results = list(r.result_or_exc)\n",
    "#         if(len(results)>0 and not results[0].applied):\n",
    "#             not_applieds[row_no] = results[0].tid\n",
    "# #             print(row_no,\"Row already exists for tid:\",results[0].tid)      \n",
    "#     else:\n",
    "# #         not_applieds[row_no] = r.result_or_exc\n",
    "#         print(row_no,\"Insertion failed with exception: \", r.result_or_exc)\n",
    "# print(\"Not inserted (already exists): \",len(not_applieds),not_applieds)\n",
    "\n",
    "for t in insert_tables:\n",
    "    print(\"Getting counts for\",t)\n",
    "    prev_time = time.time()\n",
    "    count = session.execute('select count(*) from '+t);\n",
    "    print(\"Count:\",count[0].count, tally_counts[t], tally_counts[t] - count[0].count,\", Time taken for Count query: %.2f seconds\"% (time.time()-prev_time) )\n",
    "\n",
    "\n",
    "# https://datastax.github.io/python-driver/user_defined_types.html\n",
    "# Note:  However, you must register a class to insert UDT columns with unprepared statements.\n",
    "\n",
    "\"\"\"\n",
    "Frozen value: Ensures the dict is updated entirely(NULLs allowed, but all parameters should be present), also acts as a workaround for nested collection types\n",
    "A frozen value serializes multiple components into a single value. Non-frozen types allow updates to individual fields. Cassandra treats the value of a frozen type as a blob. The entire value must be overwritten.\n",
    "\"\"\"\n",
    "\n",
    "# print(total_records,count[0].count, abs(total_records - count[0].count))\n",
    "\n",
    "session.shutdown(); # for use by cqlsh\n",
    "cluster.shutdown();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not_applieds={2773: 951604610837467136, 3020: 950878799305961472, 9514: 960301927337775104, 9844: 958851807593209856, 15754: 969360953581043712, 16162: 969361297165963264, 16278: 969361534311895040, 20671: 958126766564040704, 20803: 958126981387866112, 21704: 945081050019090432, 22358: 945081433206423552, 22396: 945081436838793216, 31615: 971172662465658880, 32194: 971173278445441024, 34112: 970810803921326080, 34692: 954865213609795584, 34910: 954865477473460224, 34988: 954865493625704448, 37475: 941820159093653504, 44091: 970086099082096640, 49572: 944354996942000128, 55415: 936383444157841408, 60292: 950154991536427008, 64977: 966462426865651712, 68258: 963925026406879232, 74102: 971898252697124864, 80051: 941095303838658560, 86902: 968999101789626368, 91561: 938195997381025792, 91877: 938196280345366528, 94693: 970448511434190848, 95226: 941457244117458944, 95824: 941457662843260928, 101775: 951241429061459968, 107326: 940732774901829632}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# i=0;\n",
    "# for row_no, tid in not_applieds.items():\n",
    "#     if(insert_records[row_no][20] == tid or insert_records[row_no+1][20] == tid ):\n",
    "#         i+=1\n",
    "#         print(i,tid,\"Conflict at row index\",row_no,insert_records[row_no][20],row_no+1,insert_records[row_no+1][20],row_no-1,insert_records[row_no-1][20])#,insert_records[row_no+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
