{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "runflag=1\n",
    "insertflag=1\n",
    "single_record=-10 # index of record\n",
    "max_num_records=120#0000\n",
    "not_applieds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"workshop_dataset/workshop_dataset1/\";\n",
    "all_files = glob.glob(data_dir+\"*.json\")\n",
    "total_files = len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 956677215197970432 to 956678066608074752\n",
      "Data columns (total 25 columns):\n",
      "author                     1000 non-null object\n",
      "author_id                  1000 non-null int64\n",
      "author_profile_image       1000 non-null object\n",
      "author_screen_name         1000 non-null object\n",
      "date                       1000 non-null object\n",
      "datetime                   1000 non-null object\n",
      "hashtags                   547 non-null object\n",
      "keywords_processed_list    779 non-null object\n",
      "lang                       1000 non-null object\n",
      "like_count                 1000 non-null int64\n",
      "location                   757 non-null object\n",
      "media_list                 123 non-null object\n",
      "mentions                   860 non-null object\n",
      "quote_count                1000 non-null int64\n",
      "quoted_source_id           46 non-null float64\n",
      "reply_count                1000 non-null int64\n",
      "replyto_source_id          34 non-null float64\n",
      "retweet_count              1000 non-null int64\n",
      "retweet_source_id          800 non-null float64\n",
      "sentiment                  1000 non-null int64\n",
      "tid                        1000 non-null int64\n",
      "tweet_text                 1000 non-null object\n",
      "type                       1000 non-null object\n",
      "url_list                   229 non-null object\n",
      "verified                   1000 non-null object\n",
      "dtypes: float64(3), int64(7), object(15)\n",
      "memory usage: 203.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file=all_files[0]\n",
    "df=pd.read_json(file,orient='index',\n",
    "                convert_dates=False, #dont convert columns to dates \n",
    "                convert_axes=False, #dont convert index to dates\n",
    "        )\n",
    "print(df.info())\n",
    "# print(df)\n",
    "# createTableString = pd.io.sql.get_schema(df, 'twitter_table')\n",
    "# print(createTableString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scanning 113 files with total of 1000 records(8.85 records/file), in 0.12 seconds(967.48 files/sec)\n"
     ]
    }
   ],
   "source": [
    "# Test the data for consistencies. \n",
    "\n",
    "# Check Identical columns in all files and\n",
    "# Note Record Counts to tally with cassandra counts later\n",
    "if(runflag == 1):\n",
    "    i=0\n",
    "    df_shapes=[]\n",
    "    all_records = pd.DataFrame()\n",
    "    time_taken = 0\n",
    "    prev_time = time.time()\n",
    "    for filename in all_files:\n",
    "        #the dtype is important for correctness. https://stackoverflow.com/questions/49627084/pandas-read-json-reads-large-integers-as-strings-incorrectly\n",
    "        df=pd.read_json(filename,orient='index',convert_dates=False,convert_axes=False,dtype={\"tid\": int})            \n",
    "        df_shapes.append(df.shape)\n",
    "        curr_set = set(df)\n",
    "        if(i>0):\n",
    "            all_records = pd.concat([all_records,df])\n",
    "            if(prev_set != curr_set):\n",
    "                runflag=1\n",
    "                print(filename, \"New columns found!\", curr_set - prev_set)\n",
    "                break\n",
    "        else:\n",
    "            all_records=df\n",
    "        runflag=0\n",
    "        if(all_records.shape[0] > max_num_records):\n",
    "            break\n",
    "        prev_set = curr_set\n",
    "        i+=1\n",
    "    time_taken= time.time()-prev_time\n",
    "\n",
    "    # Rename columns that are keywords\n",
    "    all_records = all_records.rename(columns={'date':'tweet_date','datetime':'tweet_datetime'})\n",
    "    # convert dates to datetime objects\n",
    "    all_records['tweet_date'] = pd.to_datetime(all_records['tweet_date'])\n",
    "    all_records['tweet_datetime'] = pd.to_datetime(all_records['tweet_datetime'])\n",
    "    all_records['verified']=all_records['verified'].astype(bool)\n",
    "    # convert lists to str as the python driver doesn't do it before utf-encode \n",
    "    # list_columns=['hashtags', 'keywords_processed_list', 'mentions', 'url_list']\n",
    "    # for c in list_columns:\n",
    "    #     all_records[c]=all_records[c].astype(str)\n",
    "\n",
    "total_records= sum([ x for x,y in df_shapes])\n",
    "total_records2= all_records.shape[0]\n",
    "if(total_records!=total_records2):\n",
    "    print(\"Error: Something's wrong with dataset, total counts not matching!\",total_records,total_records2)\n",
    "if(runflag==0):\n",
    "    print(\"Done scanning %d files with total of %d records(%.2f records/file), in %.2f seconds(%.2f files/sec)\" %\n",
    "          (total_files, total_records, float(total_records)/total_files,time_taken, float(total_files)/time_taken))\n",
    "\n",
    "# Ensure data types are correct in the dataframe-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.query import UNSET_VALUE\n",
    "# from cassandra.util import OrderedMap\n",
    "\n",
    "#debuggin\n",
    "all_records_tail = all_records.tail(max_num_records)\n",
    "all_columns = all_records_tail.columns.values # or list(df)\n",
    "insert_records = all_records_tail.as_matrix()\n",
    "# Handling NULLs -\n",
    "# https://www.datastax.com/dev/blog/python-driver-2-6-0-rc1-with-cassandra-2-2-features\n",
    "def handleNullInts(row):\n",
    "    for i,val in enumerate(row):\n",
    "#         if(type(val)==dict):\n",
    "#             Convert keys to integers\n",
    "#             row[i] = OrderedMap(row[i])\n",
    "        # floats to ints coz pandas uses float which supports NoneTypes too, ints can't support this in pandas!\n",
    "        # float_columns=['quoted_source_id', 'replyto_source_id', 'retweet_source_id']\n",
    "        # Changing Cassandra columns to float as NULL support required.\n",
    "        if(type(val)==float):\n",
    "            if np.isnan(val):\n",
    "#                 print(\"None Set for index\",i,val)\n",
    "                row[i]= UNSET_VALUE \n",
    "            else:                \n",
    "                row[i]= int(row[i])\n",
    "\n",
    "for record in insert_records:\n",
    "    handleNullInts(record)\n",
    "\n",
    "# print(insert_records[0])\n",
    "\n",
    "# ^^ Can use this migration method as well: https://datastax.github.io/python-driver/object_mapper.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(iquery) # Reloads modules in interactive shell\n",
    "# https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author', 'author_id', 'author_profile_image',\n",
       "       'author_screen_name', 'tweet_date', 'tweet_datetime', 'hashtags',\n",
       "       'keywords_processed_list', 'lang', 'like_count', 'location',\n",
       "       'media_list', 'mentions', 'quote_count', 'quoted_source_id',\n",
       "       'reply_count', 'replyto_source_id', 'retweet_count',\n",
       "       'retweet_source_id', 'sentiment', 'tid', 'tweet_text', 'type',\n",
       "       'url_list', 'verified'], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('author', 'author_id', 'author_profile_image', 'author_screen_name', 'hashtags', 'keywords_processed_list', 'lang', 'like_count', 'location', 'media_list', 'mentions', 'quote_count', 'quoted_source_id', 'reply_count', 'replyto_source_id', 'retweet_count', 'retweet_source_id', 'sentiment', 'tid', 'tweet_text', 'tweet_date', 'tweet_datetime', 'type', 'url_list', 'verified')\n",
      "tweets_by_tid Pok Mat Cong Codei\n",
      "120 --> 120\n",
      "\n",
      "('author', 'author_id', 'author_screen_name', 'lang', 'location', 'tid', 'tweet_text', 'tweet_date', 'tweet_datetime', 'like_count', 'author_profile_image', 'hashtags', 'keywords_processed_list', 'media_list', 'mentions', 'quote_count', 'quoted_source_id', 'reply_count', 'replyto_source_id', 'retweet_count', 'retweet_source_id', 'sentiment', 'type', 'url_list', 'verified')\n",
      "tweets_by_author Pok Mat Cong Codei\n",
      "120 --> 120\n",
      "\n",
      "('author', 'author_id', 'author_screen_name', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_author Pok Mat Cong Codei\n",
      "120 --> 120\n",
      "\n",
      "('tweet_date', 'tweet_datetime', 'tid')\n",
      "tids_by_date 2018-01-25 00:00:00\n",
      "120 --> 120\n",
      "\n",
      "('location', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_location Kota Bharu, Kelantan\n",
      "120 --> 90\n",
      "\n",
      "('hashtag', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_hashtag HappyRepublicDay\n",
      "120 --> 145\n",
      "\n",
      "('mention', 'tid', 'tweet_date', 'tweet_datetime')\n",
      "tids_by_mention MUFC_Malaysia\n",
      "120 --> 134\n"
     ]
    }
   ],
   "source": [
    "# Create individual record arrays here\n",
    "import insert_queries as iquery # local file having insert statements\n",
    "insert_tables = iquery.columns.keys()\n",
    "table_records={}\n",
    "tally_counts={}\n",
    "custom_col_tables=['tids_by_hashtag', 'tids_by_mention'];\n",
    "\n",
    "def getindex(all_columns,c):\n",
    "    return np.where(all_columns == c)[0][0]\n",
    "for t in insert_tables:    \n",
    "    columns=iquery.columns[t]\n",
    "    print('')\n",
    "    print(columns)\n",
    "    partition_key = getindex(all_columns,columns[0]+('s' if t in custom_col_tables else ''))    \n",
    "    filtered_records = insert_records[insert_records[:,partition_key] != None]\n",
    "    rem_indices = [ getindex(all_columns,c) for c in columns[1:] if c in all_columns]\n",
    "    if(t in custom_col_tables):    \n",
    "        list_records = filtered_records[:,partition_key]\n",
    "        expanded_records=[]\n",
    "        for row_no,row in enumerate(list_records):\n",
    "            for list_item in row:\n",
    "                #Works only on first col\n",
    "                expanded_records.append(np.concatenate([[list_item],filtered_records[row_no][rem_indices]]))\n",
    "        table_records[t]=expanded_records\n",
    "    else:\n",
    "        table_records[t]= filtered_records[:,[i for i in [partition_key]+rem_indices]]\n",
    "    print(t, table_records[t][0][0])\n",
    "    print(len(insert_records),\"-->\",len(table_records[t]))\n",
    "    tally_counts[t]=len(table_records[t])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i,r in enumerate(insert_records[10]):\n",
    "#     print(i+1,all_columns[i],type(r),r,\"\\n\")\n",
    "insertflag=1\n",
    "# single_record=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time taken for Insertion: 2.31 seconds\n",
      "Getting counts for tweets_by_tid\n",
      "Count: 120 120 0 , Time taken for Count query: 0.01 seconds\n",
      "Getting counts for tweets_by_author\n",
      "Count: 120 120 0 , Time taken for Count query: 0.00 seconds\n",
      "Getting counts for tids_by_author\n",
      "Count: 120 120 0 , Time taken for Count query: 0.00 seconds\n",
      "Getting counts for tids_by_date\n",
      "Count: 120 120 0 , Time taken for Count query: 0.00 seconds\n",
      "Getting counts for tids_by_location\n",
      "Count: 90 90 0 , Time taken for Count query: 0.00 seconds\n",
      "Getting counts for tids_by_hashtag\n",
      "Count: 145 145 0 , Time taken for Count query: 0.00 seconds\n",
      "Getting counts for tids_by_mention\n",
      "Count: 134 134 0 , Time taken for Count query: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent, execute_concurrent_with_args\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "session = cluster.connect('twitterdb') # keyspace name should be lowercase\n",
    "# added 'if not exists' at end\n",
    "\n",
    "# tweets_by_tid_stmt = session.prepare(iquery.statementstweets_by_tid)\n",
    "prepared_statements={}\n",
    "for t in insert_tables:\n",
    "    prepared_statements[t]=session.prepare(iquery.statements[t])\n",
    "    \n",
    "# Note that it will overwrite data without error (use 'IF NOT EXISTS' and check applied == False)\n",
    "# session.execute(statement,insert_records[0])\n",
    "\n",
    "if(insertflag==1):\n",
    "    print(\"Inserting Records..\")\n",
    "    prev_time = time.time()\n",
    "    if single_record > -1:\n",
    "#         print(\"media_list\", insert_records[single_record][11])\n",
    "        for t in insert_tables:\n",
    "            results = session.execute(prepared_statements[t],table_records[t][single_record])\n",
    "    \n",
    "        result_set=[dotdict({'success':True,'result_or_exc' : results})]\n",
    "        total_inserted=1\n",
    "    else:\n",
    "        for t in insert_tables:\n",
    "            result_set = list(execute_concurrent_with_args(session, prepared_statements[t], table_records[t], concurrency=50))\n",
    "        # One can iterate only once on this result_set!\n",
    "#         result_set = list(execute_concurrent_with_args(session, statement, insert_records, concurrency=50))\n",
    "        total_inserted = total_records\n",
    "    insertflag=0;\n",
    "    time_taken=time.time()-prev_time    \n",
    "            \n",
    "print(\"Total Time taken for Insertion: %.2f seconds\"% (time_taken) )\n",
    "# print(\"Time taken for Insertion: %.2f seconds, %.2f records/second \"% (time_taken, total_inserted/time_taken) )\n",
    "\n",
    "# print(\"Reviewing inserts..\")\n",
    "# for row_no,r in enumerate(result_set):\n",
    "#     if(r.success == True):\n",
    "#         results = list(r.result_or_exc)\n",
    "#         if(len(results)>0 and not results[0].applied):\n",
    "#             not_applieds[row_no] = results[0].tid\n",
    "# #             print(row_no,\"Row already exists for tid:\",results[0].tid)      \n",
    "#     else:\n",
    "# #         not_applieds[row_no] = r.result_or_exc\n",
    "#         print(row_no,\"Insertion failed with exception: \", r.result_or_exc)\n",
    "# print(\"Not inserted (already exists): \",len(not_applieds),not_applieds)\n",
    "\n",
    "for t in insert_tables:\n",
    "    print(\"Getting counts for\",t)\n",
    "    prev_time = time.time()\n",
    "    count = session.execute('select count(*) from '+t);\n",
    "    print(\"Count:\",count[0].count, tally_counts[t], tally_counts[t] - count[0].count,\", Time taken for Count query: %.2f seconds\"% (time.time()-prev_time) )\n",
    "\n",
    "\n",
    "# https://datastax.github.io/python-driver/user_defined_types.html\n",
    "# Note:  However, you must register a class to insert UDT columns with unprepared statements.\n",
    "\n",
    "\"\"\"\n",
    "Frozen value: Ensures the dict is updated entirely(NULLs allowed, but all parameters should be present), also acts as a workaround for nested collection types\n",
    "A frozen value serializes multiple components into a single value. Non-frozen types allow updates to individual fields. Cassandra treats the value of a frozen type as a blob. The entire value must be overwritten.\n",
    "\"\"\"\n",
    "\n",
    "# print(total_records,count[0].count, abs(total_records - count[0].count))\n",
    "\n",
    "session.shutdown(); # for use by cqlsh\n",
    "cluster.shutdown();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not_applieds={2773: 951604610837467136, 3020: 950878799305961472, 9514: 960301927337775104, 9844: 958851807593209856, 15754: 969360953581043712, 16162: 969361297165963264, 16278: 969361534311895040, 20671: 958126766564040704, 20803: 958126981387866112, 21704: 945081050019090432, 22358: 945081433206423552, 22396: 945081436838793216, 31615: 971172662465658880, 32194: 971173278445441024, 34112: 970810803921326080, 34692: 954865213609795584, 34910: 954865477473460224, 34988: 954865493625704448, 37475: 941820159093653504, 44091: 970086099082096640, 49572: 944354996942000128, 55415: 936383444157841408, 60292: 950154991536427008, 64977: 966462426865651712, 68258: 963925026406879232, 74102: 971898252697124864, 80051: 941095303838658560, 86902: 968999101789626368, 91561: 938195997381025792, 91877: 938196280345366528, 94693: 970448511434190848, 95226: 941457244117458944, 95824: 941457662843260928, 101775: 951241429061459968, 107326: 940732774901829632}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# i=0;\n",
    "# for row_no, tid in not_applieds.items():\n",
    "#     if(insert_records[row_no][20] == tid or insert_records[row_no+1][20] == tid ):\n",
    "#         i+=1\n",
    "#         print(i,tid,\"Conflict at row index\",row_no,insert_records[row_no][20],row_no+1,insert_records[row_no+1][20],row_no-1,insert_records[row_no-1][20])#,insert_records[row_no+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
