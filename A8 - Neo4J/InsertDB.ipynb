{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "runflag=1\n",
    "insertflag=1\n",
    "NULL_INT=-1\n",
    "single_record=-10 # index of record\n",
    "max_num_records=1200#0000\n",
    "not_applieds={}\n",
    "data_dir=\"workshop_dataset/workshop_dataset1/\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_files = [\"dataset.json\"]\n",
    "# glob.glob(data_dir+\"*.json\")\n",
    "file=all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scanning 1 file(s) with total of 514 records(514.00 records/file), in 0.07 seconds(13.43 files/sec)\n"
     ]
    }
   ],
   "source": [
    "# Test the data for consistencies and make it importable\n",
    "if(runflag == 1):\n",
    "    total_files=0\n",
    "    df_shapes=[]\n",
    "    all_records = pd.DataFrame()\n",
    "    time_taken = 0\n",
    "    prev_time = time.time()\n",
    "    for filename in all_files:\n",
    "        df=pd.read_json(filename,orient='index',convert_dates=False,convert_axes=False,dtype={\"tid\": int})            \n",
    "        df_shapes.append(df.shape)\n",
    "        curr_set = set(df)\n",
    "        total_files+=1\n",
    "        if(total_files==1):\n",
    "            all_records=df\n",
    "        else:\n",
    "            all_records = pd.concat([all_records,df])\n",
    "            if(prev_set != curr_set):\n",
    "                runflag=1\n",
    "                print(filename, \"New columns found!\", curr_set - prev_set)\n",
    "                break\n",
    "        runflag=0\n",
    "        if(all_records.shape[0] > max_num_records):\n",
    "            break\n",
    "        prev_set = curr_set\n",
    "    time_taken= time.time()-prev_time\n",
    "\n",
    "    # Rename columns that are keywords\n",
    "    all_records = all_records.rename(columns={'date':'tweet_date','datetime':'tweet_datetime'})\n",
    "    all_records['verified']=all_records['verified'].astype(bool)\n",
    "    all_records['tid']=all_records['tid'].astype(int)\n",
    "#     all_records['quoted_source_id']=all_records['quoted_source_id'].fillna(NULL_INT).astype(int)\n",
    "#     all_records['replyto_source_id']=all_records['replyto_source_id'].fillna(NULL_INT).astype(int)\n",
    "#     all_records['retweet_source_id']=all_records['retweet_source_id'].fillna(NULL_INT).astype(int)\n",
    "    \n",
    "\n",
    "total_records= sum([ x for x,y in df_shapes])\n",
    "total_records2= all_records.shape[0]\n",
    "if(total_records!=total_records2):\n",
    "    print(\"Error: Something's wrong with dataset, total counts not matching!\",total_records,total_records2)\n",
    "if(runflag==0):\n",
    "    print(\"Done scanning %d file(s) with total of %d records(%.2f records/file), in %.2f seconds(%.2f files/sec)\" %\n",
    "          (total_files, total_records, float(total_records)/total_files,time_taken, float(total_files)/time_taken))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "g = Graph()\n",
    "g.delete_all()\n",
    "\n",
    "#clean the graph\n",
    "insert_nodes=1\n",
    "insert_rels=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE CONSTRAINT ON (Author:Author) ASSERT Author.author_id IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Author:Author) ASSERT Author.author_screen_name IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Tweet:Tweet) ASSERT Tweet.tid IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Media:Media) ASSERT Media.media_id IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Url:Url) ASSERT Url.url IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Keyword:Keyword) ASSERT Keyword.keyword IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Location:Location) ASSERT Location.location IS UNIQUE\n",
      "CREATE CONSTRAINT ON (Hashtag:Hashtag) ASSERT Hashtag.hashtag IS UNIQUE\n"
     ]
    }
   ],
   "source": [
    "uniq_constraints={\n",
    "    'Author':[\"author_id\",\"author_screen_name\"],\n",
    "    'Tweet': [\"tid\",],\n",
    "    'Media':[ 'media_id',],\n",
    "    'Url':[ 'url', ], \n",
    "    'Keyword':[ 'keyword', ], \n",
    "    'Location':[ 'location', ], \n",
    "    'Hashtag':[ 'hashtag', ] \n",
    "}\n",
    "#Adding unique also adds index\n",
    "for k,v in uniq_constraints.items():\n",
    "    for attr in v:\n",
    "        print(\"CREATE CONSTRAINT ON ({0}:{0}) ASSERT {0}.{1} IS UNIQUE\".format(k,attr))\n",
    "        #Try catch blocks so that this code can be repeatedly run without issues\n",
    "        try:\n",
    "            g.run(\"DROP CONSTRAINT ON ({0}:{0}) ASSERT {0}.{1} IS UNIQUE\".format(k,attr))\n",
    "        except:\n",
    "            try:\n",
    "                g.run(\"CREATE CONSTRAINT ON ({0}:{0}) ASSERT {0}.{1} IS UNIQUE\".format(k,attr))\n",
    "            except:\n",
    "                g.run(\"DROP INDEX ON :{0}({1});\".format(k,attr))\n",
    "                g.run(\"CREATE CONSTRAINT ON ({0}:{0}) ASSERT {0}.{1} IS UNIQUE\".format(k,attr))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:66: DeprecationWarning: Transaction.append(...) is deprecated, use Transaction.run(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertion Time: 2.4213943481445312\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "prev_time= time.time()\n",
    "tx=graph.begin()\n",
    "media_cols=[ 'media_id', 'media_type', 'display_url', 'media_url', ]\n",
    "for index, row in all_records.iterrows():\n",
    "    params=row.to_dict()\n",
    "    \n",
    "    m_item_list=[]\n",
    "    if(params['media_list'] is not None):\n",
    "        for r in params['media_list'].values():\n",
    "            m_item_list.append([ m_item[c] for c in media_cols])\n",
    "    params['list_media_items']=m_item_list\n",
    "    statement=\"\"\"\n",
    "      MERGE (p:Author {author_id: {author_id}})\n",
    "          REMOVE p:Shadow\n",
    "          SET p.author = {author},\n",
    "              p.author_screen_name = {author_screen_name},\n",
    "              p.author_profile_image = {author_profile_image}\n",
    "          \n",
    "      MERGE (t:Tweet {tid: {tid}})\n",
    "          REMOVE p:Shadow\n",
    "          SET t.type = {type},\n",
    "              t.tweet_date = {tweet_date},\n",
    "              t.tweet_datetime = {tweet_datetime},\n",
    "              t.tweet_text = {tweet_text},\n",
    "              t.quote_count = {quote_count},\n",
    "              t.reply_count = {reply_count},\n",
    "              t.like_count = {like_count},\n",
    "              t.sentiment = {sentiment},\n",
    "              t.retweet_count = {retweet_count},\n",
    "              t.verified = {verified},\n",
    "              t.lang = {lang},\n",
    "              t.like_count = {like_count}\n",
    "      MERGE (p)-[:WROTE]->(t)\n",
    "       \n",
    "      FOREACH(locn in CASE WHEN NOT {location} is null THEN[1] ELSE [] END |        \n",
    "        MERGE (loc:Location {location: {location}})\n",
    "        MERGE (t)-[:AT]->(loc)\n",
    "      )            \n",
    "\n",
    "      FOREACH( htag IN {hashtags} |\n",
    "        MERGE (htg:Hashtag {hashtag: htag})\n",
    "        ON CREATE SET htg:Shadow\n",
    "        MERGE (t)-[:HAS_HASHTAG]->(htg)\n",
    "      ) \n",
    "      \n",
    "      FOREACH( mention IN {mentions} |\n",
    "        MERGE (mnt_node:Mention {mention: mention})\n",
    "        ON CREATE SET mnt_node:Shadow\n",
    "        MERGE (mnt_node)-[:MENTIONED_IN]->(t)\n",
    "      )\n",
    "\n",
    "      FOREACH( u IN {url_list} |\n",
    "        MERGE (url_node:Url {url: u})\n",
    "        ON CREATE SET url_node:Shadow\n",
    "        MERGE (t)-[:HAS_URL]->(url_node)\n",
    "      )\n",
    "\n",
    "      FOREACH( k IN {keywords_processed_list}|\n",
    "        MERGE (keyword_node:Keyword {keyword: k})\n",
    "        ON CREATE SET keyword_node:Shadow\n",
    "        MERGE (t)-[:HAS_KEYWORD]->(keyword_node)\n",
    "      )\n",
    "\n",
    "    \"\"\"\n",
    "    tx.append(statement, params)\n",
    "    tx.process()\n",
    "tx.commit()\n",
    "time_nodes=time.time()-prev_time\n",
    "print(\"Insertion Time:\",time_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE INDEX ON :Tweet(type);\n",
      "CREATE INDEX ON :Tweet(tweet_date);\n",
      "CREATE INDEX ON :Tweet(tweet_datetime);\n"
     ]
    }
   ],
   "source": [
    "# Adding indices may take time. And make inserts slow. Hence do after inserts    \n",
    "# Except for necessary unique ones which are done before insert\n",
    "indices={\n",
    "    #Wherever we gonna search which was not covered in unique\n",
    "    'Tweet' : ['type','tweet_date','tweet_datetime',],\n",
    "}\n",
    "for k,v in indices.items():\n",
    "    for attr in v:\n",
    "        g.run(\"CREATE INDEX ON :{0}({1});\".format(k,attr))\n",
    "        print(\"CREATE INDEX ON :{0}({1});\".format(k,attr))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_hashtags=[]\n",
    "# for i,row in all_records.iterrows():\n",
    "#     if(row.hashtags):\n",
    "#         all_hashtags+=row.hashtags\n",
    "        \n",
    "# df = pd.DataFrame([ [h] for h in all_hashtags],columns=['hash']).groupby(['hash'])\n",
    "# for hashtag,rows in df:\n",
    "#     counts=rows.groupby('location').count()\n",
    "#     for l,c in counts.iterrows():\n",
    "#         hashtag_locations[hashtag] = c.name,c.hashtag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
