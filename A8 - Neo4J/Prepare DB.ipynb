{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "runflag=1\n",
    "insertflag=1\n",
    "single_record=-10 # index of record\n",
    "max_num_records=120#00000\n",
    "not_applieds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir=\"workshop_dataset/workshop_dataset1/\";\n",
    "all_files = glob.glob(data_dir+\"*.json\")\n",
    "total_files = len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scanning 113 files with total of 1000 records(8.85 records/file), in 0.13 seconds(888.70 files/sec)\n"
     ]
    }
   ],
   "source": [
    "# Test the data for consistencies. \n",
    "\n",
    "# Check Identical columns in all files and\n",
    "# Note Record Counts to tally with cassandra counts later\n",
    "if(runflag == 1):\n",
    "    i=0\n",
    "    df_shapes=[]\n",
    "    all_records = pd.DataFrame()\n",
    "    time_taken = 0\n",
    "    prev_time = time.time()\n",
    "    for filename in all_files:\n",
    "        #the dtype is important for correctness. https://stackoverflow.com/questions/49627084/pandas-read-json-reads-large-integers-as-strings-incorrectly\n",
    "        df=pd.read_json(filename,orient='index',convert_dates=False,convert_axes=False,dtype={\"tid\": int})            \n",
    "        df_shapes.append(df.shape)\n",
    "        curr_set = set(df)\n",
    "        if(i>0):\n",
    "            all_records = pd.concat([all_records,df])\n",
    "            if(prev_set != curr_set):\n",
    "                runflag=1\n",
    "                print(filename, \"New columns found!\", curr_set - prev_set)\n",
    "                break\n",
    "        else:\n",
    "            all_records=df\n",
    "        runflag=0\n",
    "        if(all_records.shape[0] > max_num_records):\n",
    "            break\n",
    "        prev_set = curr_set\n",
    "        i+=1\n",
    "    time_taken= time.time()-prev_time\n",
    "\n",
    "    # Rename columns that are keywords\n",
    "    all_records = all_records.rename(columns={'date':'tweet_date','datetime':'tweet_datetime'})\n",
    "    # convert dates to datetime objects\n",
    "    all_records['tweet_date'] = pd.to_datetime(all_records['tweet_date'])\n",
    "    all_records['tweet_datetime'] = pd.to_datetime(all_records['tweet_datetime'])\n",
    "    all_records['verified']=all_records['verified'].astype(bool)\n",
    "    # convert lists to str as the python driver doesn't do it before utf-encode \n",
    "    # list_columns=['hashtags', 'keywords_processed_list', 'mentions', 'url_list']\n",
    "    # for c in list_columns:\n",
    "    #     all_records[c]=all_records[c].astype(str)\n",
    "\n",
    "total_records= sum([ x for x,y in df_shapes])\n",
    "total_records2= all_records.shape[0]\n",
    "if(total_records!=total_records2):\n",
    "    print(\"Error: Something's wrong with dataset, total counts not matching!\",total_records,total_records2)\n",
    "if(runflag==0):\n",
    "    print(\"Done scanning %d files with total of %d records(%.2f records/file), in %.2f seconds(%.2f files/sec)\" %\n",
    "          (total_files, total_records, float(total_records)/total_files,time_taken, float(total_files)/time_taken))\n",
    "\n",
    "# Ensure data types are correct in the dataframe-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.query import UNSET_VALUE\n",
    "# from cassandra.util import OrderedMap\n",
    "\n",
    "#debuggin\n",
    "all_records_tail = all_records.tail(max_num_records)\n",
    "all_columns = all_records_tail.columns.values # or list(df)\n",
    "insert_records = all_records_tail.as_matrix()\n",
    "# Handling NULLs -\n",
    "# https://www.datastax.com/dev/blog/python-driver-2-6-0-rc1-with-cassandra-2-2-features\n",
    "def handleNullInts(row):\n",
    "    for i,val in enumerate(row):\n",
    "#         if(type(val)==dict):\n",
    "#             Convert keys to integers\n",
    "#             row[i] = OrderedMap(row[i])\n",
    "        # floats to ints coz pandas uses float which supports NoneTypes too, ints can't support this in pandas!\n",
    "        # float_columns=['quoted_source_id', 'replyto_source_id', 'retweet_source_id']\n",
    "        # Changing Cassandra columns to float as NULL support required.\n",
    "        if(type(val)==float):\n",
    "            if np.isnan(val):\n",
    "#                 print(\"None Set for index\",i,val)\n",
    "                row[i]= UNSET_VALUE \n",
    "            else:                \n",
    "                row[i]= int(row[i])\n",
    "\n",
    "for record in insert_records:\n",
    "    handleNullInts(record)\n",
    "\n",
    "# print(insert_records[0])\n",
    "\n",
    "# ^^ Can use this migration method as well: https://datastax.github.io/python-driver/object_mapper.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "# import importlib\n",
    "# importlib.reload(iquery) # Reloads modules in interactive shell\n",
    "# https://stackoverflow.com/questions/1254370/reimport-a-module-in-python-while-interactive\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "columns={}\n",
    "# Normal table\n",
    "columns['hashtag_locations']=('hashtag', 'location' ) \n",
    "columns['hashtag_mention_list']=('hashtag', 'mentions' ,'tweet_date' ) \n",
    "# Needs completely diff function\n",
    "columns['locations_by_hashtag']=( 'hashtag', 'location' ,'location_count') \n",
    "columns['hashtag_mentions']=('hashtag', 'mention' ,'tweet_date') \n",
    "\n",
    "statements={}\n",
    "for table in columns.keys():\n",
    "\tcols=columns[table]\n",
    "\tstatements[table]=\"insert into twitterdb.\"+table+\" (\"+\",\".join(cols)+\") values (\" + \"?,\"*(len(cols)-1)+\"?\" + \") if not exists\";\n",
    "\n",
    "    \n",
    "# columns['tids_by_author']=('author_screen_name','author', 'author_id', 'tid', 'tweet_date', 'tweet_datetime' )\n",
    "iquery=dotdict({\n",
    "    'columns':columns,\n",
    "    'statements':statements\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('hashtag', 'location')\n",
      "hashtag_locations HappyRepublicDay\n",
      "120 --> 145\n",
      "\n",
      "('hashtag', 'mentions', 'tweet_date')\n",
      "hashtag_mention_list HappyRepublicDay\n",
      "120 --> 145\n"
     ]
    }
   ],
   "source": [
    "# Create individual record arrays here\n",
    "# import insert_queries as iquery # local file having insert statements\n",
    "insert_tables = iquery.columns.keys()\n",
    "table_records={}\n",
    "tally_counts={}\n",
    "custom_col_tables=['tids_by_keyword','tids_by_hashtag','hashtag_locations','hashtag_mention_list', 'tids_by_mention'];\n",
    "custom_col_listnames={\n",
    "    'keyword':'keywords_processed_list',\n",
    "    'hashtag':'hashtags',\n",
    "    'mention':'mentions'\n",
    "}\n",
    "\n",
    "def getindex(all_columns,c):\n",
    "    return np.where(all_columns == c)[0][0]\n",
    "\n",
    "def getcolname(all_columns,c):\n",
    "    if(c in custom_col_listnames.keys()):\n",
    "        return custom_col_listnames[c]\n",
    "    else:\n",
    "        return c\n",
    "special_table='locations_by_hashtag'\n",
    "special_table2='hashtag_mentions'\n",
    "exclude_tables=['hashtag_locations','hashtag_mention_list']\n",
    "\n",
    "for t in insert_tables:\n",
    "    if(t==special_table or t==special_table2):\n",
    "        continue\n",
    "    else:\n",
    "        columns=iquery.columns[t]\n",
    "        print('')\n",
    "        print(columns)\n",
    "        partition_key = getindex(all_columns,getcolname(all_columns,columns[0]))    \n",
    "        filtered_records = insert_records[insert_records[:,partition_key] != None] #No floats used, so no need to check UNSET_VALUE\n",
    "        rem_indices = [ getindex(all_columns,c) for c in columns[1:] if c in all_columns]\n",
    "        if(t in custom_col_tables):    \n",
    "            list_records = filtered_records[:,partition_key]\n",
    "            expanded_records=[]\n",
    "            for row_no,row in enumerate(list_records):\n",
    "                for list_item in row:\n",
    "                    #Works only on first col\n",
    "                    expanded_records.append(np.concatenate([[list_item],filtered_records[row_no][rem_indices]]))\n",
    "            table_records[t]=expanded_records\n",
    "        else:\n",
    "            table_records[t]= filtered_records[:,[i for i in [partition_key]+rem_indices]]\n",
    "    print(t, table_records[t][0][0])\n",
    "    print(len(insert_records),\"-->\",len(table_records[t]))\n",
    "    tally_counts[t]=len(table_records[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag     122\n",
      "location    122\n",
      "dtype: int64\n",
      "hashtag       117\n",
      "mentions      117\n",
      "tweet_date    117\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "t='hashtag_locations'\n",
    "t2='hashtag_mention_list'\n",
    "df = pd.DataFrame(table_records[t],columns=iquery.columns[t]).dropna()\n",
    "df2 = pd.DataFrame(table_records[t2],columns=iquery.columns[t2]).dropna()\n",
    "\n",
    "print(df.count())\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "locations_by_hashtag [['1YearOfRaees', 'lebanon', 6], ['1yearofRaees', 'lebanon', 1], ['BREAKING', 'california', 1], ['Beer', 'South Carolina ', 1], ['BuenJueves', 'IRREVERENCIA EN LA DISCUSIÓN Y LEALTAD EN LA ACCIÓN.', 1], ['CNN', 'Alabama', 1], ['ChangeTheRules', 'Amsterdam, The Netherlands', 1], ['Comey', 'MI', 1], ['DeepState', 'Union, KY', 1], ['Democrat', 'With James Spiritually', 1]]\n"
     ]
    }
   ],
   "source": [
    "hashtag_locations={}\n",
    "for hashtag,rows in df.groupby('hashtag'):\n",
    "    counts=rows.groupby('location').count()\n",
    "    for l,c in counts.iterrows():\n",
    "        hashtag_locations[hashtag] = c.name,c.hashtag\n",
    "\n",
    "items = list(hashtag_locations.items())\n",
    "table_records[special_table]= [ [l[0]]+list(l[1]) for l in items ]\n",
    "tally_counts[special_table]=len(table_records[special_table])\n",
    "print(tally_counts[special_table])\n",
    "print(special_table,table_records[special_table][:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "hashtag_mentions [['1YearOfRaees' 'Kemet2MkMert' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'am_jaanu' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'saaquib27' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'Seda_Artist' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'SRKCHENNAIFC' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'SaadUmer17' Timestamp('2018-01-25 00:00:00')]]\n"
     ]
    }
   ],
   "source": [
    "hashtag_mentions=[]\n",
    "for hashtag,rows in df2.groupby('hashtag'):\n",
    "#     print(hashtag)\n",
    "    tdate= list(rows.tweet_date)[0]\n",
    "    for ms in rows['mentions']:\n",
    "        for m in ms:\n",
    "            hashtag_mentions.append([hashtag,m,tdate])\n",
    "#             print(hashtag,m)\n",
    "#     print('')\n",
    "hm = pd.DataFrame(hashtag_mentions)\n",
    "table_records[special_table2] =hm.as_matrix()\n",
    "# df = hm.drop_duplicates()\n",
    "# df = hm.groupby( list(hm),as_index=False).size()\n",
    "tally_counts[special_table2]=len(table_records[special_table2])\n",
    "\n",
    "print(tally_counts[special_table2])\n",
    "print(special_table2,table_records[special_table2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Nope- df.sort_values(by=['tweet_datetime','hashtag'],ascending=[False,True])\n",
    "# df = pd.DataFrame(table_records['tids_by_hashtag'],columns=['hashtag','tid','tweet_date','tweet_datetime'])\n",
    "# date_col='tweet_date'\n",
    "# df = df[['hashtag',date_col]]\n",
    "# hashtag_counts={}\n",
    "# for day,rows in df.groupby(date_col,sort=True): #keys are sorted\n",
    "#     hashtag_counts[day] = rows['hashtag'].value_counts()\n",
    "# #     print(hashtag_counts[day])\n",
    "# #     print(rows.groupby('hashtag').count())\n",
    "\n",
    "# accum_sum = pd.DataFrame(hashtag_counts).fillna(0)\n",
    "# # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html\n",
    "# accum_sum = accum_sum.rolling(7,axis=1,min_periods=1).sum()\n",
    "# accum_sum.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insertflag=1\n",
    "# single_record=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting Records..\n",
      "locations_by_hashtag\n",
      "[['1YearOfRaees', 'lebanon', 6], ['1yearofRaees', 'lebanon', 1], ['BREAKING', 'california', 1], ['Beer', 'South Carolina ', 1], ['BuenJueves', 'IRREVERENCIA EN LA DISCUSIÓN Y LEALTAD EN LA ACCIÓN.', 1], ['CNN', 'Alabama', 1], ['ChangeTheRules', 'Amsterdam, The Netherlands', 1], ['Comey', 'MI', 1], ['DeepState', 'Union, KY', 1], ['Democrat', 'With James Spiritually', 1]]\n",
      "hashtag_mentions\n",
      "[['1YearOfRaees' 'Kemet2MkMert' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'am_jaanu' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'saaquib27' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'Seda_Artist' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'iamsrk' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'SRKCHENNAIFC' Timestamp('2018-01-25 00:00:00')]\n",
      " ['1YearOfRaees' 'SaadUmer17' Timestamp('2018-01-25 00:00:00')]]\n",
      "Total Time taken for Insertion: 0.45 seconds\n",
      "Getting counts for locations_by_hashtag\n",
      "Count: 80 80 0 , Time taken for Count query: 0.02 seconds\n",
      "Getting counts for hashtag_mentions\n",
      "Count: 107 135 28 , Time taken for Count query: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent, execute_concurrent_with_args\n",
    "cluster = Cluster(['127.0.0.1'])\n",
    "\n",
    "session = cluster.connect('twitterdb') # keyspace name should be lowercase\n",
    "# added 'if not exists' at end\n",
    "# tweets_by_tid_stmt = session.prepare(iquery.statementstweets_by_tid)\n",
    "prepared_statements={}\n",
    "for t in insert_tables:\n",
    "    if(t not in exclude_tables):\n",
    "        prepared_statements[t]=session.prepare(iquery.statements[t])\n",
    "    \n",
    "# Note that it will overwrite data without error (use 'IF NOT EXISTS' and check applied == False)\n",
    "# session.execute(statement,insert_records[0])\n",
    "\n",
    "if(insertflag==1):\n",
    "    print(\"Inserting Records..\")\n",
    "    prev_time = time.time()\n",
    "    if single_record > -1:\n",
    "#         print(\"media_list\", insert_records[single_record][11])\n",
    "        for t in insert_tables:\n",
    "            if(t not in exclude_tables):\n",
    "                print(t)\n",
    "                results = session.execute(prepared_statements[t],table_records[t][single_record])\n",
    "    \n",
    "        result_set=[dotdict({'success':True,'result_or_exc' : results})]\n",
    "        total_inserted=1\n",
    "    else:\n",
    "        for t in insert_tables:\n",
    "            if(t not in exclude_tables):\n",
    "                print(t)\n",
    "#                 print(table_records[t][:10])\n",
    "                result_set = list(execute_concurrent_with_args(session, prepared_statements[t], table_records[t], concurrency=50))\n",
    "        # One can iterate only once on this result_set!\n",
    "        # That is useful when you do pagination: https://datastax.github.io/php-driver/api/Cassandra/class.Rows/\n",
    "#         result_set = list(execute_concurrent_with_args(session, statement, insert_records, concurrency=50))\n",
    "        total_inserted = total_records\n",
    "    insertflag=0;\n",
    "    time_taken=time.time()-prev_time    \n",
    "            \n",
    "print(\"Total Time taken for Insertion: %.2f seconds\"% (time_taken) )\n",
    "# print(\"Time taken for Insertion: %.2f seconds, %.2f records/second \"% (time_taken, total_inserted/time_taken) )\n",
    "\n",
    "# print(\"Reviewing inserts..\")\n",
    "# for row_no,r in enumerate(result_set):\n",
    "#     if(r.success == True):\n",
    "#         results = list(r.result_or_exc)\n",
    "#         if(len(results)>0 and not results[0].applied):\n",
    "#             not_applieds[row_no] = results[0].tid\n",
    "# #             print(row_no,\"Row already exists for tid:\",results[0].tid)      \n",
    "#     else:\n",
    "# #         not_applieds[row_no] = r.result_or_exc\n",
    "#         print(row_no,\"Insertion failed with exception: \", r.result_or_exc)\n",
    "# print(\"Not inserted (already exists): \",len(not_applieds),not_applieds)\n",
    "\n",
    "for t in insert_tables:\n",
    "    if(t not in exclude_tables):\n",
    "        print(\"Getting counts for\",t)\n",
    "        prev_time = time.time()\n",
    "        count = session.execute('select count(*) from '+t);\n",
    "        print(\"Count:\",count[0].count, tally_counts[t], tally_counts[t] - count[0].count,\", Time taken for Count query: %.2f seconds\"% (time.time()-prev_time) )\n",
    "\n",
    "\n",
    "# https://datastax.github.io/python-driver/user_defined_types.html\n",
    "# Note:  However, you must register a class to insert UDT columns with unprepared statements.\n",
    "\n",
    "\"\"\"\n",
    "Frozen value: Ensures the dict is updated entirely(NULLs allowed, but all parameters should be present), also acts as a workaround for nested collection types\n",
    "A frozen value serializes multiple components into a single value. Non-frozen types allow updates to individual fields. Cassandra treats the value of a frozen type as a blob. The entire value must be overwritten.\n",
    "\"\"\"\n",
    "\n",
    "# print(total_records,count[0].count, abs(total_records - count[0].count))\n",
    "\n",
    "session.shutdown(); # for use by cqlsh\n",
    "cluster.shutdown();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Does retweet_count == 0 mean mentions are null ? \n",
    "# temp_all_records = all_records[all_records['retweet_count']==0]\n",
    "# temp_all_records[temp_all_records['mentions']==None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not_applieds={2773: 951604610837467136, 3020: 950878799305961472, 9514: 960301927337775104, 9844: 958851807593209856, 15754: 969360953581043712, 16162: 969361297165963264, 16278: 969361534311895040, 20671: 958126766564040704, 20803: 958126981387866112, 21704: 945081050019090432, 22358: 945081433206423552, 22396: 945081436838793216, 31615: 971172662465658880, 32194: 971173278445441024, 34112: 970810803921326080, 34692: 954865213609795584, 34910: 954865477473460224, 34988: 954865493625704448, 37475: 941820159093653504, 44091: 970086099082096640, 49572: 944354996942000128, 55415: 936383444157841408, 60292: 950154991536427008, 64977: 966462426865651712, 68258: 963925026406879232, 74102: 971898252697124864, 80051: 941095303838658560, 86902: 968999101789626368, 91561: 938195997381025792, 91877: 938196280345366528, 94693: 970448511434190848, 95226: 941457244117458944, 95824: 941457662843260928, 101775: 951241429061459968, 107326: 940732774901829632}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# i=0;\n",
    "# for row_no, tid in not_applieds.items():\n",
    "#     if(insert_records[row_no][20] == tid or insert_records[row_no+1][20] == tid ):\n",
    "#         i+=1\n",
    "#         print(i,tid,\"Conflict at row index\",row_no,insert_records[row_no][20],row_no+1,insert_records[row_no+1][20],row_no-1,insert_records[row_no-1][20])#,insert_records[row_no+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
